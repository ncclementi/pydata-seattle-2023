{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86353515-66e0-4e95-8d5c-2c770f7b0ba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import coiled\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d4256-65c7-4b2c-95e5-c65470275972",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#old default\n",
    "with dask.config.set({\"distributed.scheduler.worker-saturation\": \"inf\"}):\n",
    "    cluster_old_def = coiled.Cluster(name=\"dfalign-task-queuing-old-default\",\n",
    "                            n_workers=10,\n",
    "                            wait_for_workers=True,\n",
    "                            scheduler_options={\"idle_timeout\": \"1 hours\"}                                 \n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58558ecb-2ddd-4741-a70b-e8ef38077de4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#current default\n",
    "with dask.config.set({\"distributed.scheduler.worker-saturation\": 1.1}):\n",
    "    cluster_ws = coiled.Cluster(name=\"dfalign-task-queuing-new-default\",\n",
    "                            n_workers=10,\n",
    "                            wait_for_workers=True,\n",
    "                            scheduler_options={\"idle_timeout\": \"1 hours\"}\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3be6b2c-70a6-4543-8053-e3e2c4a82cfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client_old_def = Client(cluster_old_def)\n",
    "client_ws = Client(cluster_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29e22f5-0b69-4a05-8e48-2013c0b6663f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f1d93-bb99-4b4a-8396-1569c0d8b01c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import dask.array as da\n",
    "import distributed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.datasets import timeseries\n",
    "from dask.sizeof import sizeof\n",
    "from dask.utils import format_bytes, parse_bytes\n",
    "\n",
    "def cluster_memory(client: distributed.Client) -> int:\n",
    "    \"\"\"Total memory available on the cluster, in bytes\"\"\"\n",
    "    return int(\n",
    "        sum(w[\"memory_limit\"] for w in client.scheduler_info()[\"workers\"].values())\n",
    "    )\n",
    "\n",
    "\n",
    "def timeseries_of_size(\n",
    "    target_nbytes: int | str,\n",
    "    *,\n",
    "    start=\"2000-01-01\",\n",
    "    freq=\"1s\",\n",
    "    partition_freq=\"1d\",\n",
    "    dtypes={\"name\": str, \"id\": int, \"x\": float, \"y\": float},\n",
    "    seed=None,\n",
    "    **kwargs,\n",
    ") -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a `dask.demo.timeseries` of a target total size.\n",
    "    Same arguments as `dask.demo.timeseries`, but instead of specifying an ``end`` date,\n",
    "    you specify ``target_nbytes``. The number of partitions is set as necessary to reach\n",
    "    approximately that total dataset size. Note that you control the partition size via\n",
    "    ``freq``, ``partition_freq``, and ``dtypes``.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> timeseries_of_size(\n",
    "    ...     \"1mb\", freq=\"1s\", partition_freq=\"100s\", dtypes={\"x\": float}\n",
    "    ... ).npartitions\n",
    "    278\n",
    "    >>> timeseries_of_size(\n",
    "    ...     \"1mb\", freq=\"1s\", partition_freq=\"100s\", dtypes={i: float for i in range(10)}\n",
    "    ... ).npartitions\n",
    "    93\n",
    "    Notes\n",
    "    -----\n",
    "    The ``target_nbytes`` refers to the amount of RAM the dask DataFrame would use up\n",
    "    across all workers, as many pandas partitions.\n",
    "    This is typically larger than ``df.compute()`` would be as a single pandas\n",
    "    DataFrame. Especially with many partions, there can be significant overhead to\n",
    "    storing all the individual pandas objects.\n",
    "    Additionally, ``target_nbytes`` certainly does not correspond to the size\n",
    "    the dataset would take up on disk (as parquet, csv, etc.).\n",
    "    \"\"\"\n",
    "    if isinstance(target_nbytes, str):\n",
    "        target_nbytes = parse_bytes(target_nbytes)\n",
    "\n",
    "    start_dt = pd.to_datetime(start)\n",
    "    partition_freq_dt = pd.to_timedelta(partition_freq)\n",
    "    example_part = timeseries(\n",
    "        start=start,\n",
    "        end=start_dt + partition_freq_dt,\n",
    "        freq=freq,\n",
    "        partition_freq=partition_freq,\n",
    "        dtypes=dtypes,\n",
    "        seed=seed,\n",
    "        **kwargs,\n",
    "    )\n",
    "    \n",
    "    #catch warning generated when computing using the threaded scheduler having active distributed ones \n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    p = example_part.compute(scheduler=\"threads\")\n",
    "    partition_size = sizeof(p)\n",
    "    npartitions = round(target_nbytes / partition_size)\n",
    "    assert npartitions > 0, (\n",
    "        f\"Partition size of {format_bytes(partition_size)} > \"\n",
    "        f\"target size {format_bytes(target_nbytes)}\"\n",
    "    )\n",
    "\n",
    "    ts = timeseries(\n",
    "        start=start,\n",
    "        end=start_dt + partition_freq_dt * npartitions,\n",
    "        freq=freq,\n",
    "        partition_freq=partition_freq,\n",
    "        dtypes=dtypes,\n",
    "        seed=seed,\n",
    "        **kwargs,\n",
    "    )\n",
    "    assert ts.npartitions == npartitions\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a822221-2820-4544-b38b-9c91af6a1c85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = cluster_memory(client_old_def)\n",
    "format_bytes(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4840925-90af-49c8-81a9-6c8a74fba6bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = timeseries_of_size(\n",
    "    memory // 2,\n",
    "    start=\"2020-01-01\",\n",
    "    freq=\"600ms\",\n",
    "    partition_freq=\"12h\",\n",
    "    dtypes={i: float for i in range(100)},\n",
    ")\n",
    "\n",
    "df2 = timeseries_of_size(\n",
    "    memory // 4,\n",
    "    start=\"2010-01-01\",\n",
    "    freq=\"600ms\",\n",
    "    partition_freq=\"12h\",\n",
    "    dtypes={i: float for i in range(100)},\n",
    ")\n",
    "\n",
    "\n",
    "final = (df2 - df).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20476511-4fd5-458b-b2cc-0f3d61e49c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Old default: {client_old_def.dashboard_link}\")\n",
    "print(f\"New default with ws: {client_ws.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbf45b4-5a75-45c4-b8e9-3f1cbb455313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f_old = client_old_def.compute(final)\n",
    "f_ws = client_ws.compute(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5958a7-5150-4617-a3b8-eecd0e9cd75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# wait(client_old_def.compute(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055e7c62-26d3-4e49-98c9-4317e411e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# wait(client_ws.compute(final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ba4db-2826-4ed9-abab-b2c48d0e5b2b",
   "metadata": {},
   "source": [
    "### Old version\n",
    "```python\n",
    "%%time\n",
    "wait(client_old_def.compute(final))\n",
    "\n",
    "\n",
    "CPU times: user 408 ms, sys: 86.4 ms, total: 495 ms\n",
    "Wall time: 1min 21s # 2min 55s #1min 44s #4min 2s\n",
    "```\n",
    "\n",
    "### New default\n",
    "```python\n",
    "%%time\n",
    "wait(client_ws.compute(final))\n",
    "\n",
    "CPU times: user 225 ms, sys: 52.2 ms, total: 277 ms\n",
    "Wall time: 37.5 s\n",
    "\n",
    "```\n",
    "\n",
    "## ~2.5 - 6.5X faster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141687a2-fdb6-425d-b3b4-2a205f8d550d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client_old_def.shutdown()\n",
    "client_ws.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b856e-0f7a-438d-80a2-87092d5b2e90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
